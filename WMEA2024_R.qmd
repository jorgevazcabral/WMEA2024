---
title: "WMEA 2024"
subtitle: "Linear regression in R software: from ordinary least squares to maximum entropy estimation"
title-block-banner: "#70AD47"
title-block-banner-color: "white"
date: 2024-10-28
author:
  - name: Jorge Cabral
    id: jc
    email: jorgecabral@ua.pt
    affiliations:
      - name: Department of Mathematics, University of Aveiro, Aveiro, Portugal
        id: 1
        department: Department of Mathematics
        city: Aveiro
        country: Portugal
        url: https://www.ua.pt/pt/dmat
      
      - name: CIDMA, University of Aveiro, Aveiro, Portugal
        id: 2
        department: University of Aveiro
        city: Aveiro
        country: Portugal
        url: https://cidma.ua.pt/
    attributes:
        corresponding: true
format: 
  html:
    toc: true
    toc_float: yes
    toc-location: left
    toc-depth: 6
    theme: Sandstone
    highlight-style: Ayu-dark
    code-link: true
number-sections: true
bibliography: references.bib
editor: source
---

## Introduction

One of the most widely used data analysis techniques is univariate multiple linear regression. In general, it proposes to mathematically model the relation between a dependent variable and a set of independent variables (or predictors) in order to understand whether the latter predict and/or explain the former. In the modelling process some assumptions should be imposed [@Greene2008].\
Data transformations are sometimes applied, being one of those the standardization of the variables. This is done by subtracting the mean from each and then dividing by their respective standard deviation. The estimated coefficients (from now on referred as coefficients) are called standardized coefficients [@Bring1994].\
Consider the univariate multiple linear regression model given by
\begin{align}
 y &= X\beta + \epsilon,
\end{align}
where $y$ denotes a $(N \times 1)$ vector of noisy observations, $N \in \mathbb{N}$, $\beta$ is a $((K+1) \times 1)$ vector of unknown parameters or coefficients, $K \in \mathbb{N}$, $X$ is a known $(N \times (K+1))$ design matrix and $\epsilon$ is a $(N \times 1)$ vector of random disturbances (errors), usually assumed to have a conditional expected value of zero and representing spherical disturbances, i.e, $E\left[ \epsilon | X\right]=0$ and $E[\epsilon \epsilon'|X]=\sigma^2I$, where $I$ is a ($N \times N$) identity matrix and $\sigma^2$ is the error variance.

The univariate multiple linear regression model can be written as
\begin{align}
	y &= \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \dots + \beta_K x_{K} + \epsilon,
\end{align}
and standardizing $y$ and $x_j,j \in \left\lbrace 1,\dots,K\right\rbrace$ the following results are obtained
\begin{align}
%	y^* &= f^*(y^*) + \epsilon^* 
 y^* &= X^*b + \epsilon^*\\
	y^* &= b_0 + b_1x_1^* + b_2x_2^* + \dots + b_Kx_K^* + \epsilon^*,  
\end{align}
where
\begin{align} y_i^*&=\frac{y_i-\frac{\sum_{i=1}^{N}y_i}{N}}{\sqrt{\frac{1}{N}\sum_{i=1}^{N}\left( y_i-\frac{\sum_{i=1}^{N}y_i}{N}\right)^2}},\\ x_{ji}^*&=\frac{x_{ji}-\frac{\sum_{i=1}^{N}x_{ji}}{N}}{\sqrt{\frac{1}{N}\sum_{i=1}^{N}\left( x_{ji}-\frac{\sum_{i=1}^{N}x_{ji}}{N}\right)^2}},\\ b_j&=\frac{\sqrt{\frac{1}{N}\sum_{i=1}^{N}\left( x_{ji}-\frac{\sum_{i=1}^{N}x_{ji}}{N}\right)^2}}{\sqrt{\frac{1}{N}\sum_{i=1}^{N}\left( y_i-\frac{\sum_{i=1}^{N}y_i}{N}\right)^2}}\beta_j,  \label{stand_coef_1}
\end{align}
\noindent with $j\in \left\lbrace 1,\dots,K\right\rbrace$, and $b_0=0$. In this formulation, $b_j$ are called standardized coefficients.\

<br>

### Ordinary Least Squares

The Ordinary Least Squares (OLS) estimator of $\beta$ takes the form
\begin{align}
	\widehat{\beta}^{OLS}= \begin{bmatrix}
		\widehat{\beta}_0^{OLS}\\
		\widehat{\beta}_1^{OLS}\\
		\widehat{\beta}_2^{OLS}\\
		\vdots \\
		\widehat{\beta}_K^{OLS}
	\end{bmatrix} 
	&= \underset{\beta}{\operatorname{argmin}} \|y-X\beta\|^2\\
	&= \left(X'X\right)^{-1}X'y,
\end{align}
\noindent where $X'$ is the transpose of $X$.

<br>

### Ridge regression

The Ridge regression introduced by Hoerl and Kennard [@Hoerl1970] is an estimation procedure to handle collinearity without removing variables from the regression model. By adding a small non-negative constant (ridge or shrinkage parameter) to the diagonal of the correlation matrix of the explanatory variables, it is possible to reduce the variance of the OLS estimator through the introduction of some bias. Although the resulting estimators are biased, the biases are small enough for these estimators to be substantially more precise than the unbiased estimators.
The challenge in Ridge regression remains on the selection of the ridge parameter. One straightforward approach is based on simply plotting the coefficients against several possible values for the ridge parameter and inspecting the resulting traces.
The Ridge Regression estimator of $\lambda$ takes the form
\begin{align}
\widehat{\beta}^{ridge}&= \underset{\beta}{\operatorname{argmin}} \|y-X\beta\|^2+\lambda \|\beta\|^2 \\ 
&=(X'X+\lambda I)^{-1}X'y,
\end{align}
where $\lambda \geq 0$ denotes the ridge parameter and $I$ is a $(K \times K)$ identity matrix.
Note that when $\lambda \rightarrow 0$, the Ridge regression estimator approaches the OLS estimator whereas the Ridge regression estimator approaches the zero vector when $\lambda \rightarrow \infty$. Thus, a trade-off between variance and bias is needed.

<br>

### Generalized Maximum Entropy estimator

Golan et al [@Golan1996] generalized the Maximum Entropy formalism [@Jaynes1957] to linear inverse problems with noise, expressed in the previous chapter linear model. The idea is to treat each $\beta_j$, $j\in\left\{ 0,\dots,K\right\}$, as a discrete random variable with a compact support and $2 \leq M < \infty$ possible outcomes, and each $\epsilon_i$, $i \in \left\{ 1, \dots, N \right\}$, as a finite and discrete random variable with $2 \leq J < \infty$ possible outcomes. Assuming that both the unknown parameters and the unknown error terms may be bounded a priori, the linear model can be presented as
\begin{equation*}
	Y=XZp + Vw,
\end{equation*}
where 
\begin{equation}
	\beta=Zp= \left[ 
	\begin{array}{cccc}
		z'_1   & 0      & \cdots & {0}   \\
		0      & z'_2   & \cdots & {0}   \\
		\vdots & \vdots & \ddots & \vdots\\
		0      & 0      & \cdots & {z}'_K
	\end{array}\right]
	\left[ 
	\begin{array}{c}
		{p}_1 \\
		{p}_2 \\
		\vdots\\
		{p}_K
	\end{array}\right],
\end{equation}
with ${Z}$ a $(K \times (K\times M))$ matrix of support values and ${p}$ a $((K\times M) \times 1)$ vector of unknown probabilities, and
\begin{equation}
	\epsilon={Vw}= \left[ 
	\begin{array}{cccc}
		v'_1   & 0      & \cdots & {0}   \\
		0      & v'_2   & \cdots & {0}   \\
		\vdots & \vdots & \ddots & \vdots\\
		0      & 0      & \cdots & v'_N
	\end{array}\right]
	\left[ \begin{array}{c}
		w_1 \\
		w_2 \\
		\vdots\\
		w_N
	\end{array}\right],
\end{equation}
with $V$ a $(N \times (N\times J))$ matrix of support values and $w$ a $((N\times J) \times 1)$ vector of unknown probabilities.
For the linear regression model, the Generalized Maximum Entropy (GME) estimator is given by
	\begin{equation}
		\hat{\beta}^{GME}(Z,V) = \underset{p,w}{\operatorname{argmax}}
		\left\{-p' \ln p - w' \ln w \right\},
		\label{Golan631}
	\end{equation}
	subject to the model constraint
	\begin{equation*}
		Y=XZp + Vw,
	\end{equation*}
	and the additivity constraints for $p$ and $w$, respectively,
	\begin{equation*}
		\begin{array}{c}
			1_K=(I_K \otimes 1'_M)p,\\
			1_N=(I_N \otimes 1'_J)w,
		\end{array}
	\end{equation*}
where $\otimes$ represents the Kronecker product, ${1}$ is a column vector of ones with a specific dimension, ${I}$ is an identity matrix with a specific dimension and ${Z}$ and ${V}$ are the matrices of supports, and ${p}>{0}$ and ${w}>{0}$ are probability vectors to be estimated.
The GME estimator generates the optimal probability vectors $\widehat{{p}}$ and $\widehat{{w}}$ that can be used to form point estimates of the unknown parameters and the unknown random errors. Since the objective function is strictly concave in the interior of the additivity constraint set, a unique solution for the GME estimator is guaranteed if the intersection of the model and the additivity constraint sets is non-empty [@Golan1996].
The supports in matrices ${Z}$ and ${V}$ are defined as being closed and bounded intervals within which each parameter or error is restricted to lie, implying that researchers need to provide exogenous information (which, unfortunately, it is not always available). This is considered the main weakness of the GME estimator [@Caputo2008]. Golan et al [@Golan1996] discuss these issues in the case of minimal prior information: for the unknown parameters, the authors recommend the use of wide bounds (this is naturally subjective) for the supports in ${Z}$, without extreme risk consequences; for the unknown errors, the authors suggest the use of the three-sigma rule with a sample scale parameter [@Pukelsheim1994]. The number of points $M$ in the supports is less controversial and are usually used in the literature between 3 and 7 points, since there is likely no significant improvement in the estimation with more points in the supports. The three-sigma rule, considering the standard deviation of the noisy observations and $J=3$ points in the supports is usually adopted.

<br>

## Setting up

### R software

 > "R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS." --- [R software](https://www.r-project.org/) [@RCore2024]

Follow [https://cran.r-project.org/](https://cran.r-project.org/) to install it.

![](CRAN.png)  
<br>

### R Studio

> "RStudio is an integrated development environment (IDE) designed to support multiple languages, including both R and Python." --- [RStudio](https://docs.posit.co/ide/user/)

Follow [https://posit.co/downloads/](https://posit.co/downloads/) to install it.

![](RStudio.png)  

<br>

### Packages

During the workshop, several R packages will be necessary.
Open R Studio and create a new Quarto document.

![](NewFile.png)

![](NewFile2.png)
<br>
Insert the following code inside the code chunk. 
```{r, echo=TRUE,warning=FALSE,message=FALSE}
# install.packages("devtools")
library(devtools)
# install.packages("ggcorrplot")
library(ggcorrplot)
# install.packages("ggpubr")
library(ggpubr)
# install.packages("MLmetrics")
library(MLmetrics)
# install.packages("glmnet")
library(glmnet)
# devtools::install_github("jorgevazcabral/GCE",
#                          build_vignettes = TRUE,
#                          build_manual = TRUE)
library(GCE)
#install.packages("AER")
library(AER)
```

[https://github.com/jorgevazcabral](https://github.com/jorgevazcabral)
![](Github.png)  

<br>

## Simulated Data

<br>

### Generate Data

```{r,echo=TRUE}
n = 100 #Number of individuals
intercept.beta = 0 #Intercept
y.gen.cont.beta = c(3, 6, 9) #Coefficients used for generating y
cont.k = 0 #Number of continuous variables not used for generating y #use also 5
condnumber = 1 #Condition number of the X matrix #use also 10 and 100

simulated_data <-
  gdata_GCE(
    n = n,
    cont.k = cont.k, 
    y.gen.cont.k = length(y.gen.cont.beta),
    y.gen.cont.beta = y.gen.cont.beta,
    intercept.beta = intercept.beta,
    Xgenerator.method = "svd",
    condnumber = condnumber,
    Xgenerator.seed = 28102024,
    error.dist = "normal",
    error.dist.mean = 0,
    error.dist.sd = 1,
    dataframe = TRUE)
```

```{r,echo=TRUE}
simulated_data_coef <- c(intercept.beta,
                   rep(0,cont.k),
                   y.gen.cont.beta)
simulated_data[1:5,]
```

```{r,echo=TRUE}
(simulated_data_cor <- 
  cor(simulated_data,
      method = "pearson"))

ggcorrplot::ggcorrplot(simulated_data_cor,
                       lab = TRUE)
```

<br>

### Estimation

<br>

#### OLS

```{r,echo=TRUE}
sd_model_OLS <- 
  lm(data = simulated_data,
     y ~ . #A typical model has the form response ~ terms #or y ~ .
     )

```

```{r,echo=TRUE}
summary(sd_model_OLS)
(sd_model_OLS_coef <- coef(sd_model_OLS))
```

```{r,echo=TRUE}
(sd_model_OLS_RMSE <-
  MLmetrics::RMSE(fitted(sd_model_OLS),
                  simulated_data$y))

(sd_model_OLS_beta_RMSE <-
  MLmetrics::RMSE(sd_model_OLS_coef,
                  simulated_data_coef))
```

<br>

##### Assumptions

```{r,echo=TRUE,fig.width=9,fig.height=9}
par(mfrow = c(2,2))
plot(sd_model_OLS)
par(mfrow = c(1,1))
```

```{r,echo=TRUE,message=FALSE,fig.width=9,fig.height=9}
p <- sjPlot::plot_model(sd_model_OLS,
                   type = "diag")
ggpubr::ggarrange(p[[1]],
                  p[[2]],
                  p[[3]],
                  p[[4]])
```

```{r,echo=TRUE,fig.width=9,fig.height=12}
performance::check_model(sd_model_OLS)
```


```{r,echo=TRUE}
#Breusch-Pagan test
# The Breusch-Pagan test fits a linear regression model to the residuals of a linear regression model (by default the same explanatory variables are taken as in the main regression model) and rejects if too much of the variance is explained by the additional explanatory variables.

lmtest::bptest(sd_model_OLS)
```

```{r,echo=TRUE}
#Durbin-Watson test

#The Durbin-Watson test has the null hypothesis that the autocorrelation of the disturbances is 0.
lmtest::dwtest(sd_model_OLS)
```

<br>

#### Ridge

```{r,echo=TRUE}
sd_model_ridge <- 
  glmnet::cv.glmnet(
    x = as.matrix(simulated_data[,-ncol(simulated_data)]),
    y = simulated_data[,ncol(simulated_data)] ,
    standardize = TRUE,
    alpha = 0,
    lambda = 10^seq(-3,3,by=0.1),
    nfolds = 5,
    type.measure = "default")
```

```{r,echo=TRUE}
plot(sd_model_ridge)
```

```{r,echo=TRUE}
plot(sd_model_ridge$glmnet.fit,
     xvar = "lambda")
abline(v = log(sd_model_ridge$lambda.min),
       lty = 3)
abline(v = log(sd_model_ridge$lambda.1se),
       lty = 3)
```

```{r,echo=TRUE}
coef(sd_model_ridge,
     s = "lambda.min")
coef(sd_model_ridge,
     s = "lambda.1se")

sd_model_ridge_coef <-
  t(as.matrix(coef(sd_model_ridge,
                   s = "lambda.min"))) 
```

```{r,echo=TRUE}
(sd_model_ridge_RMSE <-
  MLmetrics::RMSE(predict(sd_model_ridge,
                          as.matrix(simulated_data[,-ncol(simulated_data)]),
                          s = "lambda.min"),
                  simulated_data$y))

(sd_model_ridge_beta_RMSE <-
  MLmetrics::RMSE(sd_model_ridge_coef,
                  simulated_data_coef))
```

<br>

#### GCE

```{r,echo=TRUE,fig.width=9,fig.height=9}
sd_model_GCE <- 
  GCE::gce(data = simulated_data,
           y ~ .,
           int.one.prior = "ridge",
           int.one.method = "adaptive",
           model = TRUE,
           ci.B = 10,
           seed = 28102024
      )
```

```{r,echo=TRUE}
summary(sd_model_GCE)
sd_model_GCE_coef <- coef(sd_model_GCE) 
```

```{r,echo=TRUE}
(sd_model_GCE_RMSE <-
  MLmetrics::RMSE(fitted(sd_model_GCE),
                  simulated_data$y))

(sd_model_GCE_beta_RMSE <-
  MLmetrics::RMSE(sd_model_GCE_coef,
                  simulated_data_coef))
```

```{r,echo=TRUE}
par(mfrow = c(2,2))
plot(sd_model_GCE)
par(mfrow = c(1,1))
```

<br>

#### All

```{r,echo=TRUE}
data.frame(rbind(
  cbind(
    "TRUE" = simulated_data_coef, 
    OLS = sd_model_OLS_coef,
    ridge = as.numeric(sd_model_ridge_coef),
    GCE = sd_model_GCE_coef
  ),
  beta_RMSE = c(
    0,
    sd_model_OLS_beta_RMSE,
    sd_model_ridge_beta_RMSE,
    sd_model_GCE_beta_RMSE
  ),
  RMSE = c(
    0,
    sd_model_OLS_RMSE,
    sd_model_ridge_RMSE,
    sd_model_GCE_RMSE)
))
```

<br>

## Real Data

```{r,echo=TRUE}
which_data <- 1

if (which_data == 1) {
data("Electricity1970")
real_data <- Electricity1970[,c(2:8,1)] 
} else if (which_data == 2) {
 data("HousePrices")
 real_data <- HousePrices[1:200,c(2,3,4,1)] 
}
names(real_data)[ncol(real_data)] <- "y"

set.seed(112233)
ind <- sample(1:nrow(real_data),floor(0.7*nrow(real_data)))
real_data_train <- real_data[ind,]
real_data_test <- real_data[-ind,]

ggcorrplot::ggcorrplot(cor(real_data_train),
                       lab = TRUE)
```

```{r,echo=TRUE}
base::kappa(real_data_train,
            exact = TRUE)
```

```{r,echo=TRUE}
rd_model_OLS <- 
  lm(data = real_data_train,
     y ~ . )

rd_model_ridge <- 
  glmnet::cv.glmnet(
    x = as.matrix(real_data_train[,-ncol(real_data_train)]),
    y = real_data_train[,ncol(real_data_train)] ,
    standardize = TRUE,
    alpha = 0,
    lambda = 10^seq(-3,3,by=0.1),
    nfolds = 5,
    type.measure = "default")

rd_model_GCE <- 
  GCE::gce(data = real_data_train,
           y ~ .,
           int.one.prior = "ridge",
           int.one.method = "adaptive",
           model = TRUE,
           seed = 28102024
      )

```

```{r,echo=TRUE}
data.frame(rbind(
  cbind(
    OLS = coef(rd_model_OLS),
    ridge = as.numeric(t(as.matrix(coef(rd_model_ridge,
                                        s="lambda.min")))),
    GCE = coef(rd_model_GCE)
  ),
  RMSE = c(
    MLmetrics::RMSE(
      predict(rd_model_OLS,
              real_data_test[,-ncol(real_data_test)]),
      real_data_test$y),
    MLmetrics::RMSE(
      predict(rd_model_ridge,
              as.matrix(real_data_test[,-ncol(real_data_test)]),
              s = "lambda.min"),
      real_data_test$y),
    MLmetrics::RMSE(
      as.matrix(cbind(1,
                      real_data_test[,
                                     -ncol(real_data_test)]
                      )) %*% coef(rd_model_GCE),
                    real_data_test$y))
))

```
